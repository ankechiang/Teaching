{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction (Vectorization)\n",
    "1. [Approach1: Bag of Words](#Approach1:-Bag-of-Words)\n",
    "2. [Approach2: Term Frequency - Inverse Document Frequency (TF-IDF)](#Approach2:-Term-Frequency---Inverse-Document-Frequency-(TF-IDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "article:1\tlifestyle governs mobile choice faster better funkier hardware alone going help phone firms sell handsets research suggests instead phone firms keen get customers pushing technology sake consumers far interested handsets fit lifestyle screen size onboard memory chip inside shows depth study handset maker ericsson historically industry much focus using technology said dr michael bjorn senior advisor mobile media ericsson consumer enterprise lab stop saying technologies change lives said try speak consumers language help see fits told bbc news website study ericsson interviewed 14 000 mobile phone owners ways use phone people habits remain said dr bjorn move activity mobile phone much convenient way one good example diary writing among younger people said diaries always popular mobile phone especially one equipped camera helps keep different form youngsters use text messages also reflects desire chat keep contact friends lets slightly changed way dr bjorn said although consumers always use phone sheer variety new handset technologies make possible gradually drive new habits lifestyles ericsson research shown consumers divide different tribes use phones different ways dr bjorn said groups dubbed pioneers materialists interested trying new things behind start many trends phone use instance said older people using sms much five years ago younger users often children ageing mobile owners encouraged older people try could keep touch another factor governing speed change mobile phone use simple speed new devices bought pioneers materialists 25 people handsets new innovations cameras consumers stop worrying send picture message person end able see significant number users passed use new innovations tends take dr bjorn said early reports camera phone usage japan seemed imply innovation going flop however said 45 japanese people ericsson questioned use camera phone least month 2003 figure 29 similarly across europe numbers people taking snaps cameras starting rise 2003 4 people uk took phonecam snap least month figure 14 similar rises seen many european nations dr bjorn said people also used camera phones different ways film even digital cameras usage patterns digital cameras almost exactly replacing usage patterns analogue cameras said digital cameras tend used significant events weddings holidays birthdays contrast said camera phones used much capture moment woven everyday life\n",
      "\n",
      "article:2\tfrench honour director parker british film director sir alan parker made officer order arts letters one france highest cultural honours sir alan received decoration paris wednesday french culture minister renaud donnedieu de vabres explored possibilities film immense talent mr de vabres said presented award parker praised french films saying hollywood created modern cinema uses commodity told minister honoured thus distinguished france flag carrier cinema throughout world sir alan films include oscar winning fame plus midnight express commitments founding member director guild great britain former chairman uk film council board british film institute work campaigns shown us artist occupies essential place contemporary society mr de vabres said dreams show us links weave question world mirror work also cited director 2003 film life david gale kevin spacey played man death row proof veritable artistic commitment death sentence\n",
      "\n",
      "article:3\tfockers fuel festive film chart comedy meet fockers topped festive box office north america setting new record christmas day sequel took 44 7m 23 2m 24 26 december according studio estimates took 19 1m 9 9m christmas day alone highest takings day box office history meet fockers sequel ben stiller comedy meet parents also starring robert de niro blythe danner dustin hoffman barbra streisand despite success meet fockers takings 26 5 2003 figures blamed christmas falling weekend year christmas falls weekend bad business said paul dergarabedian president exhibitor relations compiles box office statistics weekend top 12 films took estimated 121 9m 63 3m compared 165 8m 86 1m last year third lord rings film dominated box office meet fockers knocked last week top film lemony snicket series unfortunate events third place 12 5m 6 5m comedy fat albert co written bill cosby entered chart second place opening christmas day taking 12 7m 6 6m aviator starring leonardo dicaprio howard hughes took 9 4m expanding 40 1 796 cinemas christmas day\n"
     ]
    }
   ],
   "source": [
    "sample_documents = [\"lifestyle governs mobile choice faster better funkier hardware alone going help phone firms sell handsets research suggests instead phone firms keen get customers pushing technology sake consumers far interested handsets fit lifestyle screen size onboard memory chip inside shows depth study handset maker ericsson historically industry much focus using technology said dr michael bjorn senior advisor mobile media ericsson consumer enterprise lab stop saying technologies change lives said try speak consumers language help see fits told bbc news website study ericsson interviewed 14 000 mobile phone owners ways use phone people habits remain said dr bjorn move activity mobile phone much convenient way one good example diary writing among younger people said diaries always popular mobile phone especially one equipped camera helps keep different form youngsters use text messages also reflects desire chat keep contact friends lets slightly changed way dr bjorn said although consumers always use phone sheer variety new handset technologies make possible gradually drive new habits lifestyles ericsson research shown consumers divide different tribes use phones different ways dr bjorn said groups dubbed pioneers materialists interested trying new things behind start many trends phone use instance said older people using sms much five years ago younger users often children ageing mobile owners encouraged older people try could keep touch another factor governing speed change mobile phone use simple speed new devices bought pioneers materialists 25 people handsets new innovations cameras consumers stop worrying send picture message person end able see significant number users passed use new innovations tends take dr bjorn said early reports camera phone usage japan seemed imply innovation going flop however said 45 japanese people ericsson questioned use camera phone least month 2003 figure 29 similarly across europe numbers people taking snaps cameras starting rise 2003 4 people uk took phonecam snap least month figure 14 similar rises seen many european nations dr bjorn said people also used camera phones different ways film even digital cameras usage patterns digital cameras almost exactly replacing usage patterns analogue cameras said digital cameras tend used significant events weddings holidays birthdays contrast said camera phones used much capture moment woven everyday life\",\n",
    "                   \"french honour director parker british film director sir alan parker made officer order arts letters one france highest cultural honours sir alan received decoration paris wednesday french culture minister renaud donnedieu de vabres explored possibilities film immense talent mr de vabres said presented award parker praised french films saying hollywood created modern cinema uses commodity told minister honoured thus distinguished france flag carrier cinema throughout world sir alan films include oscar winning fame plus midnight express commitments founding member director guild great britain former chairman uk film council board british film institute work campaigns shown us artist occupies essential place contemporary society mr de vabres said dreams show us links weave question world mirror work also cited director 2003 film life david gale kevin spacey played man death row proof veritable artistic commitment death sentence\", \n",
    "                   \"fockers fuel festive film chart comedy meet fockers topped festive box office north america setting new record christmas day sequel took 44 7m 23 2m 24 26 december according studio estimates took 19 1m 9 9m christmas day alone highest takings day box office history meet fockers sequel ben stiller comedy meet parents also starring robert de niro blythe danner dustin hoffman barbra streisand despite success meet fockers takings 26 5 2003 figures blamed christmas falling weekend year christmas falls weekend bad business said paul dergarabedian president exhibitor relations compiles box office statistics weekend top 12 films took estimated 121 9m 63 3m compared 165 8m 86 1m last year third lord rings film dominated box office meet fockers knocked last week top film lemony snicket series unfortunate events third place 12 5m 6 5m comedy fat albert co written bill cosby entered chart second place opening christmas day taking 12 7m 6 6m aviator starring leonardo dicaprio howard hughes took 9 4m expanding 40 1 796 cinemas christmas day\"]\n",
    "for i, txt in enumerate(sample_documents):\n",
    "    print(\"\\narticle:{}\\t{}\".format(i+1, txt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach1: Bag of Words\n",
    "### scikit-learn library provides CountVectorizer class to perform this vectorization process. Firstly, Identify Unique words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector vocabulary - {'lifestyle': 236, 'governs': 186, 'mobile': 257, 'choice': 79, 'faster': 158, 'better': 55, 'funkier': 180, 'hardware': 194, 'alone': 37, 'going': 183, 'help': 195, 'phone': 290, 'firms': 165, 'sell': 330, 'handsets': 193, 'research': 316, 'suggests': 366, 'instead': 217, 'keen': 223, 'get': 182, 'customers': 103, 'pushing': 305, 'technology': 372, 'sake': 323, 'consumers': 92, 'far': 157, 'interested': 219, 'fit': 166, 'screen': 325, 'size': 346, 'onboard': 276, 'memory': 250, 'chip': 78, 'inside': 215, 'shows': 340, 'depth': 111, 'study': 364, 'handset': 192, 'maker': 243, 'ericsson': 137, 'historically': 198, 'industry': 212, 'much': 263, 'focus': 172, 'using': 397, 'said': 322, 'dr': 126, 'michael': 253, 'bjorn': 58, 'senior': 332, 'advisor': 31, 'media': 247, 'consumer': 91, 'enterprise': 135, 'lab': 227, 'stop': 361, 'saying': 324, 'technologies': 371, 'change': 73, 'lives': 239, 'try': 387, 'speak': 354, 'language': 228, 'see': 327, 'fits': 167, 'told': 380, 'bbc': 52, 'news': 266, 'website': 404, 'interviewed': 220, '14': 3, '000': 0, 'owners': 281, 'ways': 402, 'use': 393, 'people': 288, 'habits': 191, 'remain': 312, 'move': 261, 'activity': 30, 'convenient': 96, 'way': 401, 'one': 277, 'good': 184, 'example': 148, 'diary': 117, 'writing': 414, 'among': 42, 'younger': 418, 'diaries': 116, 'always': 40, 'popular': 298, 'especially': 138, 'equipped': 136, 'camera': 67, 'helps': 196, 'keep': 224, 'different': 119, 'form': 173, 'youngsters': 419, 'text': 375, 'messages': 252, 'also': 38, 'reflects': 310, 'desire': 113, 'chat': 76, 'contact': 93, 'friends': 178, 'lets': 233, 'slightly': 347, 'changed': 74, 'although': 39, 'sheer': 337, 'variety': 399, 'new': 265, 'make': 242, 'possible': 300, 'gradually': 187, 'drive': 128, 'lifestyles': 237, 'shown': 339, 'divide': 123, 'tribes': 386, 'phones': 292, 'groups': 189, 'dubbed': 129, 'pioneers': 294, 'materialists': 246, 'trying': 388, 'things': 376, 'behind': 53, 'start': 357, 'many': 245, 'trends': 385, 'instance': 216, 'older': 275, 'sms': 348, 'five': 168, 'years': 417, 'ago': 33, 'users': 395, 'often': 274, 'children': 77, 'ageing': 32, 'encouraged': 132, 'could': 98, 'touch': 384, 'another': 44, 'factor': 153, 'governing': 185, 'speed': 355, 'simple': 344, 'devices': 115, 'bought': 62, '25': 10, 'innovations': 214, 'cameras': 68, 'worrying': 412, 'send': 331, 'picture': 293, 'message': 251, 'person': 289, 'end': 133, 'able': 27, 'significant': 341, 'number': 269, 'passed': 285, 'tends': 374, 'take': 367, 'early': 131, 'reports': 315, 'usage': 392, 'japan': 221, 'seemed': 328, 'imply': 210, 'innovation': 213, 'flop': 170, 'however': 207, '45': 17, 'japanese': 222, 'questioned': 307, 'least': 230, 'month': 260, '2003': 7, 'figure': 161, '29': 12, 'similarly': 343, 'across': 29, 'europe': 142, 'numbers': 270, 'taking': 368, 'snaps': 350, 'starting': 358, 'rise': 318, 'uk': 389, 'took': 381, 'phonecam': 291, 'snap': 349, 'similar': 342, 'rises': 319, 'seen': 329, 'european': 143, 'nations': 264, 'used': 394, 'film': 163, 'even': 144, 'digital': 120, 'patterns': 286, 'almost': 36, 'exactly': 147, 'replacing': 314, 'analogue': 43, 'tend': 373, 'events': 145, 'weddings': 405, 'holidays': 201, 'birthdays': 57, 'contrast': 95, 'capture': 70, 'moment': 259, 'woven': 413, 'everyday': 146, 'life': 235, 'french': 177, 'honour': 203, 'director': 121, 'parker': 284, 'british': 65, 'sir': 345, 'alan': 34, 'made': 241, 'officer': 273, 'order': 279, 'arts': 47, 'letters': 234, 'france': 176, 'highest': 197, 'cultural': 101, 'honours': 205, 'received': 308, 'decoration': 110, 'paris': 283, 'wednesday': 406, 'culture': 102, 'minister': 255, 'renaud': 313, 'donnedieu': 125, 'de': 107, 'vabres': 398, 'explored': 151, 'possibilities': 299, 'immense': 209, 'talent': 370, 'mr': 262, 'presented': 302, 'award': 49, 'praised': 301, 'films': 164, 'hollywood': 202, 'created': 100, 'modern': 258, 'cinema': 81, 'uses': 396, 'commodity': 88, 'honoured': 204, 'thus': 379, 'distinguished': 122, 'flag': 169, 'carrier': 71, 'throughout': 378, 'world': 411, 'include': 211, 'oscar': 280, 'winning': 409, 'fame': 156, 'plus': 297, 'midnight': 254, 'express': 152, 'commitments': 87, 'founding': 175, 'member': 249, 'guild': 190, 'great': 188, 'britain': 64, 'former': 174, 'chairman': 72, 'council': 99, 'board': 61, 'institute': 218, 'work': 410, 'campaigns': 69, 'us': 391, 'artist': 45, 'occupies': 271, 'essential': 139, 'place': 295, 'contemporary': 94, 'society': 352, 'dreams': 127, 'show': 338, 'links': 238, 'weave': 403, 'question': 306, 'mirror': 256, 'cited': 83, 'david': 105, 'gale': 181, 'kevin': 225, 'spacey': 353, 'played': 296, 'man': 244, 'death': 108, 'row': 321, 'proof': 304, 'veritable': 400, 'artistic': 46, 'commitment': 86, 'sentence': 333, 'fockers': 171, 'fuel': 179, 'festive': 160, 'chart': 75, 'comedy': 85, 'meet': 248, 'topped': 383, 'box': 63, 'office': 272, 'north': 268, 'america': 41, 'setting': 336, 'record': 309, 'christmas': 80, 'day': 106, 'sequel': 334, '44': 16, '7m': 23, '23': 8, '2m': 13, '24': 9, '26': 11, 'december': 109, 'according': 28, 'studio': 363, 'estimates': 141, '19': 5, '1m': 6, '9m': 26, 'takings': 369, 'history': 199, 'ben': 54, 'stiller': 360, 'parents': 282, 'starring': 356, 'robert': 320, 'niro': 267, 'blythe': 60, 'danner': 104, 'dustin': 130, 'hoffman': 200, 'barbra': 51, 'streisand': 362, 'despite': 114, 'success': 365, 'figures': 162, 'blamed': 59, 'falling': 154, 'weekend': 408, 'year': 416, 'falls': 155, 'bad': 50, 'business': 66, 'paul': 287, 'dergarabedian': 112, 'president': 303, 'exhibitor': 149, 'relations': 311, 'compiles': 90, 'statistics': 359, 'top': 382, '12': 1, 'estimated': 140, '121': 2, '63': 20, '3m': 14, 'compared': 89, '165': 4, '8m': 25, '86': 24, 'last': 229, 'third': 377, 'lord': 240, 'rings': 317, 'dominated': 124, 'knocked': 226, 'week': 407, 'lemony': 231, 'snicket': 351, 'series': 335, 'unfortunate': 390, '5m': 19, 'fat': 159, 'albert': 35, 'co': 84, 'written': 415, 'bill': 56, 'cosby': 97, 'entered': 134, 'second': 326, 'opening': 278, '6m': 21, 'aviator': 48, 'leonardo': 232, 'dicaprio': 118, 'howard': 206, 'hughes': 208, '4m': 18, 'expanding': 150, '40': 15, '796': 22, 'cinemas': 82}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Identify Unique words\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sample_documents)\n",
    "print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Vectorization: find the frequency count of each unique word, and zero otherwise. Each article becomes a vector of fixed size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector shape: (3, 420)\n",
      "\n",
      "article vector\n",
      " [[1 0 0 ... 1 2 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 3 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(sample_documents)\n",
    "# summarize encoded vector\n",
    "print(f'vector shape: {vector.shape}\\n')\n",
    "print(f'article vector\\n {vector.toarray()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach2: Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "- Term Frequency: Simply finds out the frequency of a word in document.\n",
    "- Inverse Document Frequency: Assigns a lower weight to the words which appear most frequently. It basically depicts the rarity of the word in all documents.\n",
    "### scikit-learn library provides TfidfVectorizer class to perform this vectorization process. Firstly, Identify Unique words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector vocabulary - {'lifestyle': 236, 'governs': 186, 'mobile': 257, 'choice': 79, 'faster': 158, 'better': 55, 'funkier': 180, 'hardware': 194, 'alone': 37, 'going': 183, 'help': 195, 'phone': 290, 'firms': 165, 'sell': 330, 'handsets': 193, 'research': 316, 'suggests': 366, 'instead': 217, 'keen': 223, 'get': 182, 'customers': 103, 'pushing': 305, 'technology': 372, 'sake': 323, 'consumers': 92, 'far': 157, 'interested': 219, 'fit': 166, 'screen': 325, 'size': 346, 'onboard': 276, 'memory': 250, 'chip': 78, 'inside': 215, 'shows': 340, 'depth': 111, 'study': 364, 'handset': 192, 'maker': 243, 'ericsson': 137, 'historically': 198, 'industry': 212, 'much': 263, 'focus': 172, 'using': 397, 'said': 322, 'dr': 126, 'michael': 253, 'bjorn': 58, 'senior': 332, 'advisor': 31, 'media': 247, 'consumer': 91, 'enterprise': 135, 'lab': 227, 'stop': 361, 'saying': 324, 'technologies': 371, 'change': 73, 'lives': 239, 'try': 387, 'speak': 354, 'language': 228, 'see': 327, 'fits': 167, 'told': 380, 'bbc': 52, 'news': 266, 'website': 404, 'interviewed': 220, '14': 3, '000': 0, 'owners': 281, 'ways': 402, 'use': 393, 'people': 288, 'habits': 191, 'remain': 312, 'move': 261, 'activity': 30, 'convenient': 96, 'way': 401, 'one': 277, 'good': 184, 'example': 148, 'diary': 117, 'writing': 414, 'among': 42, 'younger': 418, 'diaries': 116, 'always': 40, 'popular': 298, 'especially': 138, 'equipped': 136, 'camera': 67, 'helps': 196, 'keep': 224, 'different': 119, 'form': 173, 'youngsters': 419, 'text': 375, 'messages': 252, 'also': 38, 'reflects': 310, 'desire': 113, 'chat': 76, 'contact': 93, 'friends': 178, 'lets': 233, 'slightly': 347, 'changed': 74, 'although': 39, 'sheer': 337, 'variety': 399, 'new': 265, 'make': 242, 'possible': 300, 'gradually': 187, 'drive': 128, 'lifestyles': 237, 'shown': 339, 'divide': 123, 'tribes': 386, 'phones': 292, 'groups': 189, 'dubbed': 129, 'pioneers': 294, 'materialists': 246, 'trying': 388, 'things': 376, 'behind': 53, 'start': 357, 'many': 245, 'trends': 385, 'instance': 216, 'older': 275, 'sms': 348, 'five': 168, 'years': 417, 'ago': 33, 'users': 395, 'often': 274, 'children': 77, 'ageing': 32, 'encouraged': 132, 'could': 98, 'touch': 384, 'another': 44, 'factor': 153, 'governing': 185, 'speed': 355, 'simple': 344, 'devices': 115, 'bought': 62, '25': 10, 'innovations': 214, 'cameras': 68, 'worrying': 412, 'send': 331, 'picture': 293, 'message': 251, 'person': 289, 'end': 133, 'able': 27, 'significant': 341, 'number': 269, 'passed': 285, 'tends': 374, 'take': 367, 'early': 131, 'reports': 315, 'usage': 392, 'japan': 221, 'seemed': 328, 'imply': 210, 'innovation': 213, 'flop': 170, 'however': 207, '45': 17, 'japanese': 222, 'questioned': 307, 'least': 230, 'month': 260, '2003': 7, 'figure': 161, '29': 12, 'similarly': 343, 'across': 29, 'europe': 142, 'numbers': 270, 'taking': 368, 'snaps': 350, 'starting': 358, 'rise': 318, 'uk': 389, 'took': 381, 'phonecam': 291, 'snap': 349, 'similar': 342, 'rises': 319, 'seen': 329, 'european': 143, 'nations': 264, 'used': 394, 'film': 163, 'even': 144, 'digital': 120, 'patterns': 286, 'almost': 36, 'exactly': 147, 'replacing': 314, 'analogue': 43, 'tend': 373, 'events': 145, 'weddings': 405, 'holidays': 201, 'birthdays': 57, 'contrast': 95, 'capture': 70, 'moment': 259, 'woven': 413, 'everyday': 146, 'life': 235, 'french': 177, 'honour': 203, 'director': 121, 'parker': 284, 'british': 65, 'sir': 345, 'alan': 34, 'made': 241, 'officer': 273, 'order': 279, 'arts': 47, 'letters': 234, 'france': 176, 'highest': 197, 'cultural': 101, 'honours': 205, 'received': 308, 'decoration': 110, 'paris': 283, 'wednesday': 406, 'culture': 102, 'minister': 255, 'renaud': 313, 'donnedieu': 125, 'de': 107, 'vabres': 398, 'explored': 151, 'possibilities': 299, 'immense': 209, 'talent': 370, 'mr': 262, 'presented': 302, 'award': 49, 'praised': 301, 'films': 164, 'hollywood': 202, 'created': 100, 'modern': 258, 'cinema': 81, 'uses': 396, 'commodity': 88, 'honoured': 204, 'thus': 379, 'distinguished': 122, 'flag': 169, 'carrier': 71, 'throughout': 378, 'world': 411, 'include': 211, 'oscar': 280, 'winning': 409, 'fame': 156, 'plus': 297, 'midnight': 254, 'express': 152, 'commitments': 87, 'founding': 175, 'member': 249, 'guild': 190, 'great': 188, 'britain': 64, 'former': 174, 'chairman': 72, 'council': 99, 'board': 61, 'institute': 218, 'work': 410, 'campaigns': 69, 'us': 391, 'artist': 45, 'occupies': 271, 'essential': 139, 'place': 295, 'contemporary': 94, 'society': 352, 'dreams': 127, 'show': 338, 'links': 238, 'weave': 403, 'question': 306, 'mirror': 256, 'cited': 83, 'david': 105, 'gale': 181, 'kevin': 225, 'spacey': 353, 'played': 296, 'man': 244, 'death': 108, 'row': 321, 'proof': 304, 'veritable': 400, 'artistic': 46, 'commitment': 86, 'sentence': 333, 'fockers': 171, 'fuel': 179, 'festive': 160, 'chart': 75, 'comedy': 85, 'meet': 248, 'topped': 383, 'box': 63, 'office': 272, 'north': 268, 'america': 41, 'setting': 336, 'record': 309, 'christmas': 80, 'day': 106, 'sequel': 334, '44': 16, '7m': 23, '23': 8, '2m': 13, '24': 9, '26': 11, 'december': 109, 'according': 28, 'studio': 363, 'estimates': 141, '19': 5, '1m': 6, '9m': 26, 'takings': 369, 'history': 199, 'ben': 54, 'stiller': 360, 'parents': 282, 'starring': 356, 'robert': 320, 'niro': 267, 'blythe': 60, 'danner': 104, 'dustin': 130, 'hoffman': 200, 'barbra': 51, 'streisand': 362, 'despite': 114, 'success': 365, 'figures': 162, 'blamed': 59, 'falling': 154, 'weekend': 408, 'year': 416, 'falls': 155, 'bad': 50, 'business': 66, 'paul': 287, 'dergarabedian': 112, 'president': 303, 'exhibitor': 149, 'relations': 311, 'compiles': 90, 'statistics': 359, 'top': 382, '12': 1, 'estimated': 140, '121': 2, '63': 20, '3m': 14, 'compared': 89, '165': 4, '8m': 25, '86': 24, 'last': 229, 'third': 377, 'lord': 240, 'rings': 317, 'dominated': 124, 'knocked': 226, 'week': 407, 'lemony': 231, 'snicket': 351, 'series': 335, 'unfortunate': 390, '5m': 19, 'fat': 159, 'albert': 35, 'co': 84, 'written': 415, 'bill': 56, 'cosby': 97, 'entered': 134, 'second': 326, 'opening': 278, '6m': 21, 'aviator': 48, 'leonardo': 232, 'dicaprio': 118, 'howard': 206, 'hughes': 208, '4m': 18, 'expanding': 150, '40': 15, '796': 22, 'cinemas': 82}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(sample_documents)\n",
    "# summarize\n",
    "print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Vectorization: find the TF-IDF of each unique word, and zero otherwise. Each article becomes a vector of fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector shape: (3, 420)\n",
      "\n",
      "article vector\n",
      " [[0.03222937 0.         0.         ... 0.03222937 0.06445873 0.03222937]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.16646059 0.05548686 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(sample_documents)\n",
    "# summarize encoded vector\n",
    "print(f'vector shape: {vector.shape}\\n')\n",
    "print(f'article vector\\n {vector.toarray()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#loading_the_stop_words_from_nltk_library_\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def txt_preprocessing(total_text, index, column, df):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        \n",
    "        #replace_every_special_char_with_space\n",
    "        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n",
    "        \n",
    "        #replace_multiple_spaces_with_single_space\n",
    "        total_text = re.sub('\\s+',' ', total_text)\n",
    "        \n",
    "        #converting_all_the_chars_into_lower_case\n",
    "        total_text = total_text.lower()\n",
    "        \n",
    "        for word in total_text.split():\n",
    "        #if_the_word_is_a_not_a_stop_word_then_retain_that_word_from_the_data\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        \n",
    "        df[column][index] = string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt_preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d45fe65f6010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtxt_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'txt_preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "for index, row in train_data.iterrows():\n",
    "    if type(row['Text']) is str:\n",
    "        txt_preprocessing(row['Text'], index, 'Text', train_data)\n",
    "        print(row['Text'])\n",
    "    else:\n",
    "        print(\"THERE IS NO TEXT DESCRIPTION FOR ID :\",index)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['Category'].isin(['entertainment','tech'])].to_csv('./learn-ai-bbc/train.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018</td>\n",
       "      <td>qpr keeper day heads for preston queens park r...</td>\n",
       "      <td>1018</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319</td>\n",
       "      <td>software watching while you work software that...</td>\n",
       "      <td>1319</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1138</td>\n",
       "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
       "      <td>1138</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>india s reliance family feud heats up the ongo...</td>\n",
       "      <td>459</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
       "      <td>1020</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  ArticleId  \\\n",
       "0       1018  qpr keeper day heads for preston queens park r...       1018   \n",
       "1       1319  software watching while you work software that...       1319   \n",
       "2       1138  d arcy injury adds to ireland woe gordon d arc...       1138   \n",
       "3        459  india s reliance family feud heats up the ongo...        459   \n",
       "4       1020  boro suffer morrison injury blow middlesbrough...       1020   \n",
       "\n",
       "        Category  \n",
       "0          sport  \n",
       "1           tech  \n",
       "2       business  \n",
       "3  entertainment  \n",
       "4       politics  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in result.iterrows():\n",
    "    if type(row['Text']) is str:\n",
    "        txt_preprocessing(row['Text'], index, 'Text', result)\n",
    "    else:\n",
    "        print(\"THERE IS NO TEXT DESCRIPTION FOR ID :\",index)\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b01579a96155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ArticleId'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entertainment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./learn-ai-bbc/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "header = ['ArticleId','Text','Category']\n",
    "subset = result[result['Category'].isin(['entertainment','tech'])]\n",
    "s = pd.Series(subset.unique())\n",
    "s.to_csv('./learn-ai-bbc/test.csv', columns = header, index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
